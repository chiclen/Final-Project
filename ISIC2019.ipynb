{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSQDabxQicT5",
        "outputId": "cee8c800-1c39-4459-c487-4f13c4a45984"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow Version: 2.18.0\n",
            "Mounted at /content/drive\n",
            "Data loaded successfully. Number of samples: 24703\n",
            "Model loaded successfully.\n",
            "Processed 100/24703 images\n",
            "Processed 300/24703 images\n",
            "Processed 400/24703 images\n",
            "Processed 500/24703 images\n",
            "Processed 600/24703 images\n",
            "Processed 700/24703 images\n",
            "Processed 800/24703 images\n",
            "Processed 900/24703 images\n",
            "Processed 1100/24703 images\n",
            "Processed 1300/24703 images\n",
            "Processed 1500/24703 images\n",
            "Processed 1700/24703 images\n",
            "Processed 2000/24703 images\n",
            "Processed 2100/24703 images\n",
            "Processed 2200/24703 images\n",
            "Processed 2300/24703 images\n",
            "Processed 2400/24703 images\n",
            "Processed 2600/24703 images\n",
            "Processed 2700/24703 images\n",
            "Processed 2800/24703 images\n",
            "Processed 3400/24703 images\n",
            "Processed 3500/24703 images\n",
            "Processed 3900/24703 images\n",
            "Processed 4300/24703 images\n",
            "Processed 4400/24703 images\n",
            "Processed 4500/24703 images\n",
            "Processed 4600/24703 images\n",
            "Processed 4800/24703 images\n",
            "Processed 4900/24703 images\n",
            "Processed 5000/24703 images\n",
            "Processed 5400/24703 images\n",
            "Processed 5500/24703 images\n",
            "Processed 5800/24703 images\n",
            "Processed 5900/24703 images\n",
            "Processed 6000/24703 images\n",
            "Processed 6300/24703 images\n",
            "Processed 6400/24703 images\n",
            "Processed 6500/24703 images\n",
            "Processed 6600/24703 images\n",
            "Processed 6800/24703 images\n",
            "Processed 6900/24703 images\n",
            "Processed 7000/24703 images\n",
            "Processed 7100/24703 images\n",
            "Processed 7300/24703 images\n",
            "Processed 7400/24703 images\n",
            "Processed 7500/24703 images\n",
            "Processed 7600/24703 images\n",
            "Processed 7700/24703 images\n",
            "Processed 7900/24703 images\n",
            "Processed 8000/24703 images\n",
            "Processed 8400/24703 images\n",
            "Processed 8500/24703 images\n",
            "Processed 8600/24703 images\n",
            "Processed 8700/24703 images\n",
            "Processed 8900/24703 images\n",
            "Processed 9000/24703 images\n",
            "Processed 9100/24703 images\n",
            "Processed 9200/24703 images\n",
            "Processed 9500/24703 images\n",
            "Processed 9600/24703 images\n",
            "Processed 9700/24703 images\n",
            "Processed 9800/24703 images\n",
            "Processed 9900/24703 images\n",
            "Processed 10000/24703 images\n",
            "Processed 10200/24703 images\n",
            "Processed 10400/24703 images\n",
            "Processed 10500/24703 images\n",
            "Processed 10600/24703 images\n",
            "Processed 10700/24703 images\n",
            "Processed 10900/24703 images\n",
            "Processed 11000/24703 images\n",
            "Processed 11100/24703 images\n",
            "Processed 11200/24703 images\n",
            "Processed 12000/24703 images\n",
            "Processed 12100/24703 images\n",
            "Processed 12200/24703 images\n",
            "Processed 12900/24703 images\n",
            "Processed 13100/24703 images\n",
            "Processed 13400/24703 images\n",
            "Processed 13500/24703 images\n",
            "Processed 13600/24703 images\n",
            "Processed 13700/24703 images\n",
            "Processed 13900/24703 images\n",
            "Processed 14100/24703 images\n",
            "Processed 14300/24703 images\n",
            "Processed 14400/24703 images\n",
            "Processed 14600/24703 images\n",
            "Processed 14700/24703 images\n",
            "Processed 14800/24703 images\n",
            "Processed 14900/24703 images\n",
            "Processed 15000/24703 images\n",
            "Processed 15200/24703 images\n",
            "Processed 15300/24703 images\n",
            "Processed 15600/24703 images\n",
            "Processed 15700/24703 images\n",
            "Processed 16000/24703 images\n",
            "Processed 16100/24703 images\n",
            "Processed 16400/24703 images\n",
            "Processed 16500/24703 images\n",
            "Processed 16700/24703 images\n",
            "Processed 16900/24703 images\n",
            "Processed 17400/24703 images\n",
            "Processed 17500/24703 images\n",
            "Processed 17700/24703 images\n",
            "Processed 17800/24703 images\n",
            "Processed 18100/24703 images\n",
            "Processed 18200/24703 images\n",
            "Processed 18400/24703 images\n",
            "Processed 18600/24703 images\n",
            "Processed 18800/24703 images\n",
            "Processed 19000/24703 images\n",
            "Processed 19100/24703 images\n",
            "Processed 19300/24703 images\n",
            "Processed 19500/24703 images\n",
            "Processed 19900/24703 images\n",
            "Processed 20000/24703 images\n",
            "Processed 20100/24703 images\n",
            "Processed 20300/24703 images\n",
            "Processed 20700/24703 images\n",
            "Processed 20800/24703 images\n",
            "Processed 20900/24703 images\n",
            "Processed 21100/24703 images\n",
            "Processed 21300/24703 images\n",
            "Processed 21500/24703 images\n",
            "Processed 21600/24703 images\n",
            "Processed 21700/24703 images\n",
            "Processed 21900/24703 images\n",
            "Processed 22000/24703 images\n",
            "Processed 22200/24703 images\n",
            "Processed 22500/24703 images\n",
            "Processed 22600/24703 images\n",
            "Processed 22800/24703 images\n",
            "Processed 22900/24703 images\n",
            "Processed 23000/24703 images\n",
            "Processed 23300/24703 images\n",
            "Processed 23600/24703 images\n",
            "Processed 23700/24703 images\n",
            "Processed 23800/24703 images\n",
            "Processed 24000/24703 images\n",
            "Processed 24100/24703 images\n",
            "Processed 24300/24703 images\n",
            "Processed 24400/24703 images\n",
            "Processed 24500/24703 images\n",
            "Processed 24600/24703 images\n",
            "Predictions saved to /content/drive/MyDrive/Colab Notebooks/data/per_image_predictions_isic2019.csv\n",
            "\n",
            "Accuracy Rate by Skin Lesion Type (%):\n",
            "true_label\n",
            "akiec    55.77\n",
            "bcc      63.91\n",
            "bkl      75.80\n",
            "df       78.29\n",
            "mel      71.61\n",
            "nv       61.46\n",
            "vasc     80.77\n",
            "dtype: float64\n",
            "\n",
            "Overall Accuracy: 69.66%\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import os\n",
        "from google.colab import drive\n",
        "import time\n",
        "\n",
        "# Check TensorFlow version\n",
        "print(\"TensorFlow Version:\", tf.__version__)\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Define paths (adjust these based on your Google Drive structure)\n",
        "model_path = '/content/drive/MyDrive/Colab Notebooks/data/final_optimized_model.keras'\n",
        "data_file_path = '/content/drive/MyDrive/Colab Notebooks/data/ISIC_2019.csv'  # Path to the provided data file\n",
        "image_dir = '/content/drive/MyDrive/Colab Notebooks/data/ISIC_2019_Training_Input/'  # Path to the image directory\n",
        "output_csv_path = '/content/drive/MyDrive/Colab Notebooks/data/per_image_predictions_isic2019.csv'  # Path to save predictions\n",
        "\n",
        "# Load the data file\n",
        "data_df = pd.read_csv(data_file_path)\n",
        "print(\"Data loaded successfully. Number of samples:\", len(data_df))\n",
        "\n",
        "# Define class labels (matching app.py)\n",
        "class_labels = ['akiec', 'bcc', 'bkl', 'df', 'mel', 'nv', 'vasc']\n",
        "label_mapping = {\n",
        "    'MEL': 'mel',\n",
        "    'NV': 'nv',\n",
        "    'BCC': 'bcc',\n",
        "    'AK': 'akiec',\n",
        "    'BKL': 'bkl',\n",
        "    'DF': 'df',\n",
        "    'VASC': 'vasc'\n",
        "}\n",
        "\n",
        "# Define the focal loss function (matching app.py, without tf.keras decorator)\n",
        "def focal_loss(alpha=None, gamma=1.0):\n",
        "    if alpha is None:\n",
        "        alpha = [0.1347, 0.0857, 0.0401, 0.3832, 0.0396, 0.0066, 0.3102]  # From app.py\n",
        "    def loss(y_true, y_pred):\n",
        "        y_true = tf.cast(y_true, tf.float32)\n",
        "        y_pred = tf.clip_by_value(y_pred, 1e-7, 1.0 - 1e-7)\n",
        "        ce = -y_true * tf.math.log(y_pred)\n",
        "        weight = tf.convert_to_tensor(alpha, dtype=tf.float32) * tf.pow(1.0 - y_pred, gamma)\n",
        "        return tf.reduce_mean(weight * ce)\n",
        "    return loss\n",
        "\n",
        "# Function to preprocess a single image (matching app.py)\n",
        "def preprocess_image(image_path, target_size=(300, 300)):\n",
        "    #print(f\"Preprocessing started for {image_path}\")\n",
        "    if not os.path.exists(image_path):\n",
        "        raise ValueError(f\"File not found: {image_path}\")\n",
        "\n",
        "    # Read and decode the image (same as app.py)\n",
        "    img_raw = tf.io.read_file(image_path)\n",
        "    img = tf.image.decode_jpeg(img_raw, channels=3)\n",
        "\n",
        "    # Resize the image\n",
        "    img = tf.image.resize(img, target_size)\n",
        "\n",
        "    # Apply EfficientNet preprocessing (same as app.py)\n",
        "    img = tf.keras.applications.efficientnet.preprocess_input(img)\n",
        "\n",
        "    # Debug: Print shape and min/max values\n",
        "    img_np = img.numpy()\n",
        "    #print(\"Image shape:\", img_np.shape)\n",
        "    #print(\"Min, Max:\", img_np.min(), img_np.max())\n",
        "\n",
        "    # Add batch dimension\n",
        "    img_np = np.expand_dims(img_np, axis=0)\n",
        "    #print(\"Preprocessing completed\")\n",
        "    return img_np\n",
        "\n",
        "# Function to get the ground truth label\n",
        "def get_ground_truth(row):\n",
        "    for col in label_mapping.keys():\n",
        "        if row[col] == 1:\n",
        "            return label_mapping[col]\n",
        "    return None\n",
        "\n",
        "# Load the pre-trained model with the custom loss\n",
        "custom_objects = {'loss': focal_loss(alpha=[0.1347, 0.0857, 0.0401, 0.3832, 0.0396, 0.0066, 0.3102], gamma=1.0)}\n",
        "model = tf.keras.models.load_model(model_path, custom_objects=custom_objects)\n",
        "print(\"Model loaded successfully.\")\n",
        "\n",
        "# List to store predictions\n",
        "results = []\n",
        "\n",
        "# Process each image one by one\n",
        "for idx, row in data_df.iterrows():\n",
        "    image_id = row['image']\n",
        "    image_path = os.path.join(image_dir, f\"{image_id}.jpg\")\n",
        "\n",
        "    if not os.path.exists(image_path):\n",
        "       # print(f\"Image not found: {image_path}\")\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        img_array = preprocess_image(image_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Error preprocessing image {image_path}: {e}\")\n",
        "        continue\n",
        "\n",
        "    true_label = get_ground_truth(row)\n",
        "    if true_label is None:\n",
        "        print(f\"No ground truth label found for {image_id}\")\n",
        "        continue\n",
        "\n",
        "    start_time = time.time()\n",
        "    prediction = model.predict(img_array, verbose=0)\n",
        "    inference_time = time.time() - start_time\n",
        "\n",
        "    predicted_class_idx = np.argmax(prediction[0])\n",
        "    predicted_label = class_labels[predicted_class_idx]\n",
        "    confidence = float(prediction[0][predicted_class_idx])\n",
        "\n",
        "    results.append({\n",
        "        'image_id': image_id,\n",
        "        'true_label': true_label,\n",
        "        'predicted_label': predicted_label,\n",
        "        'confidence': confidence,\n",
        "        'inference_time': inference_time\n",
        "    })\n",
        "\n",
        "    if (idx + 1) % 100 == 0:\n",
        "        print(f\"Processed {idx + 1}/{len(data_df)} images\")\n",
        "\n",
        "# Save results to CSV\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv(output_csv_path, index=False)\n",
        "print(f\"Predictions saved to {output_csv_path}\")\n",
        "\n",
        "# Calculate and display per-class accuracy\n",
        "def calculate_per_class_accuracy(df):\n",
        "    correct_predictions = df[df['true_label'] == df['predicted_label']]\n",
        "    per_class_accuracy = correct_predictions.groupby('true_label').size() / df.groupby('true_label').size()\n",
        "    return per_class_accuracy * 100\n",
        "\n",
        "per_class_acc = calculate_per_class_accuracy(results_df)\n",
        "print(\"\\nAccuracy Rate by Skin Lesion Type (%):\")\n",
        "print(per_class_acc.fillna(0).round(2))\n",
        "\n",
        "# Calculate and display overall accuracy\n",
        "overall_accuracy = (results_df['true_label'] == results_df['predicted_label']).mean() * 100\n",
        "print(f\"\\nOverall Accuracy: {overall_accuracy:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "OIDHI_J8tE_h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
